
ccc: 

我以為 GPT 是用 MLM + NSP 當 loss，結果搞錯了
BERT 才是 MLM + NSP 

GPT1 除語言模型外，有4 種 Task (Refined Task)
1. Classification
2. Entailment
3. Similarity 
4. Multiple Choice

GPT 的語言模型是用 n-gram 的馬可夫模型， P(x[t] | x[t-1], ... x[t-n])
...


* [YouTube: CodeEmporium: BERT Neural Network - EXPLAINED!](https://www.youtube.com/watch?v=xI0HHN5XKDo)
