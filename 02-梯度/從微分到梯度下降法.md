# 從微分到梯度下降法

* [深度學習的數學核心 (從微分到梯度下降法) (ipynb)](https://colab.research.google.com/drive/1R6zgKCVBe2HfTUR__NOXCuvToRbBTftZ?usp=sharing#scrollTo=fAx8-JfDgtVY)

## 微分 (單變數)

* [diff.py](01-diff/diff.py)

```py
def diff(f, x):
    df = f(x+dx)-f(x)
    return df/dx
```

## 偏微分 (多變數)

* [npGradient.py](02-gradient/npGradient.py)

```py
# 函數 f 對變數 p[k] 的偏微分: df / dp[k]
def df(f, p, k):
    p1 = p.copy()
    p1[k] = p[k]+step
    return (f(p1) - f(p)) / step
```


## 梯度

* [npGradient.py](02-gradient/npGradient.py)

```py
# 函數 f 在點 p 上的梯度
def grad(f, p):
    gp = p.copy()
    for k in range(len(p)):
        gp[k] = df(f, p, k)
    return gp
```

## 梯度下降法

* [gd1.py](03-gd/gd1.py)

```py
# 使用梯度下降法尋找函數最低點
def gradientDescendent(f, p0, step=0.01):
    p = p0.copy()
    i = 0
    while (True):
        i += 1
        fp = f(p)
        gp = grad(f, p) # 計算梯度 gp
        glen = norm(gp) # norm = 梯度的長度 (步伐大小)
        print('{:d}:p={:s} f(p)={:.3f} gp={:s} glen={:.5f}'.format(i, str(p), fp, str(gp), glen))
        if glen < 0.00001:  # 如果步伐已經很小了，那麼就停止吧！
            break
        gstep = np.multiply(gp, -1*step) # gstep = 逆梯度方向的一小步
        p +=  gstep # 向 gstep 方向走一小步
    return p # 傳回最低點！
```

## 反傳遞演算法

* [net.py](04-net/net.py)

```py
class Gate:
    # ...
    def backward(self):
        x, y, o, gfx, gfy = self.x, self.y, self.o, self.gfx, self.gfy
        x.g += gfx(x.v,y.v) * o.g
        y.g += gfy(x.v,y.v) * o.g
    # ...

class Net:
    # ...
    def backward(self): # 反向傳遞計算梯度 
        self.o.g = 1 # 設定輸出節點 o 的梯度為 1
        for gate in reversed(self.gates): # 反向傳遞計算每個節點 Node 的梯度 g
            gate.backward()
    # ...

```
