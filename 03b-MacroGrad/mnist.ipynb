{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRXjapBDdvk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Value:\n",
        "    \"\"\" stores a value and its gradient \"\"\"\n",
        "\n",
        "    def __init__(self, data, _children=(), _op=''):\n",
        "        self.data = data\n",
        "        self.grad = 0\n",
        "        # internal variables used for autograd graph construction\n",
        "        self._backward = lambda: None\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += out.grad\n",
        "            other.grad += out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "            \n",
        "            \n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "    \n",
        "    \n",
        "        \n",
        "        \n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "        out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "        def _backward():\n",
        "            \n",
        "            \n",
        "            self.grad += (other * self.data**(other-1)) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def relu(self):\n",
        "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (out.data > 0) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "    def matmul(self,other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(np.matmul(self.data , other.data), (self, other), 'matmul')\n",
        "        def _backward():\n",
        "            self.grad += np.dot(out.grad,other.data.T)\n",
        "            other.grad += np.dot(self.data.T,out.grad)\n",
        "            \n",
        "            \n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "    def softmax(self):\n",
        "\n",
        "        out =  Value(np.exp(self.data) / np.sum(np.exp(self.data), axis=1)[:, None], (self,), 'softmax')\n",
        "        softmax = out.data\n",
        "        def _backward():\n",
        "            self.grad += (out.grad - np.reshape(\n",
        "            np.sum(out.grad * softmax, 1),\n",
        "            [-1, 1]\n",
        "              )) * softmax\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def log(self):\n",
        "        out = Value(np.log(self.data),(self,),'log')\n",
        "        def _backward():\n",
        "            self.grad += out.grad/self.data\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "    \n",
        "    \n",
        "    def reduce_sum(self,axis = None):\n",
        "        out = Value(np.sum(self.data,axis = axis), (self,), 'REDUCE_SUM')\n",
        "        \n",
        "        def _backward():\n",
        "            output_shape = np.array(self.data.shape)\n",
        "            output_shape[axis] = 1\n",
        "            tile_scaling = self.data.shape // output_shape\n",
        "            grad = np.reshape(out.grad, output_shape)\n",
        "            self.grad += np.tile(grad, tile_scaling)\n",
        "            \n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self):\n",
        "\n",
        "        # topological order all of the children in the graph\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "\n",
        "        # go one variable at a time and apply the chain rule to get its gradient\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            #print(v)\n",
        "            v._backward()\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * -1\n",
        "\n",
        "    def __radd__(self, other): # other + self\n",
        "        return self + other\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)\n",
        "\n",
        "    def __rsub__(self, other): # other - self\n",
        "        return other + (-self)\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        return self * other\n",
        "\n",
        "    def __truediv__(self, other): # self / other\n",
        "        return self * other**-1\n",
        "\n",
        "    def __rtruediv__(self, other): # other / self\n",
        "        return other * self**-1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRWaB--7d4zs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "781ad608-7c43-44f2-bd47-0af43aaeb5af"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import keras\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfi2rgohd-8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6cba906a-5b08-49d4-95dc-5dc00b781f3d"
      },
      "source": [
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
        "train_images = np.asarray(x_train, dtype=np.float32) / 255.0\n",
        "test_images = np.asarray(x_test, dtype=np.float32) / 255.0\n",
        "train_images = train_images.reshape(60000,784)\n",
        "test_images = test_images.reshape(10000,784)\n",
        "y_train = keras.utils.to_categorical(y_train)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcdeKXE2e0T7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_loss(X,Y,W):\n",
        "  \n",
        "  return -(1/X.shape[0])*np.sum(np.sum(Y*np.log(np.exp(np.matmul(X,W)) / np.sum(np.exp(np.matmul(X,W)),axis=1)[:, None]),axis = 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnjjkOcYd-_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8f40e4a3-0890-4b6c-ae0a-93a56ae42130"
      },
      "source": [
        "batch_size = 32\n",
        "steps = 20000\n",
        "Wb = Value(np.random.randn(784,10))# new initialized weights for gradient descent\n",
        "for step in range(steps):\n",
        "  ri = np.random.permutation(train_images.shape[0])[:batch_size]\n",
        "  Xb, yb = Value(train_images[ri]), Value(y_train[ri])\n",
        "  y_predW = Xb.matmul(Wb)\n",
        "  probs = y_predW.softmax()\n",
        "  log_probs = probs.log()\n",
        "\n",
        "  zb = yb*log_probs\n",
        "\n",
        "  outb = zb.reduce_sum(axis = 1)\n",
        "  finb = -outb.reduce_sum()  #cross entropy loss\n",
        "  finb.backward()\n",
        "  if step%1000==0:\n",
        "    loss = calculate_loss(train_images,y_train,Wb.data)\n",
        "    print(f'loss in step {step} is {loss}')\n",
        "  Wb.data = Wb.data- 0.01*Wb.grad\n",
        "  Wb.grad = 0\n",
        "loss = calculate_loss(train_images,y_train,Wb.data)\n",
        "print(f'loss in final step {step+1} is {loss}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss in step 0 is 14.66993156549896\n",
            "loss in step 1000 is 0.7113508102269721\n",
            "loss in step 2000 is 0.5513684546051388\n",
            "loss in step 3000 is 0.4923705250128872\n",
            "loss in step 4000 is 0.447646185649078\n",
            "loss in step 5000 is 0.4289956266224471\n",
            "loss in step 6000 is 0.4168623457998252\n",
            "loss in step 7000 is 0.39165025181336055\n",
            "loss in step 8000 is 0.375665702033964\n",
            "loss in step 9000 is 0.3693183183725304\n",
            "loss in step 10000 is 0.36187979791186087\n",
            "loss in step 11000 is 0.34847209528718887\n",
            "loss in step 12000 is 0.3375193624274038\n",
            "loss in step 13000 is 0.3344601314571491\n",
            "loss in step 14000 is 0.3287148193350862\n",
            "loss in step 15000 is 0.3234608682723891\n",
            "loss in step 16000 is 0.3198315181097232\n",
            "loss in step 17000 is 0.3079286393710635\n",
            "loss in step 18000 is 0.31029328505114506\n",
            "loss in step 19000 is 0.31085469616921374\n",
            "loss in final step 20000 is 0.31003849539628836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_bw9LyVeGPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "912ac575-977c-497b-85b7-5c0ddc5206ab"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(f'accuracy on test data is {accuracy_score(np.argmax(np.matmul(test_images,Wb.data),axis = 1),y_test)*100} %')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy on test data is 91.11 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}