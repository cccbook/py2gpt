# 第一章 深度學習簡介

* [How do neural networks work?](https://www.youtube.com/watch?v=fkqZyYo_ebs)
* [But what *is* a Neural Network? - THE MATH YOU SHOULD KNOW!](https://www.youtube.com/watch?v=oB3gmT8GAgI)

## 1.1 什麼是深度學習

深度學習是一種機器學習的方法，通過建立多層神經網絡來實現對大量複雜數據的學習和識別。它基於人工神經網絡的發展，使用反向傳播算法對大量數據進行訓練，進而實現各種智能應用，例如圖像識別、語音識別、自然語言處理等。

深度學習的特點在於它能夠自動提取數據中的特徵，並根據這些特徵進行分類或預測。相比傳統的機器學習方法，深度學習不需要對數據進行手動特徵提取，使得模型的設計更加自動化和高效化。

深度學習模型一般由多個神經網絡層組成，每一層由多個神經元組成，神經元之間通過權重相連。模型通過將數據通過神經網絡進行前向傳播，得到輸出結果，並根據輸出和實際結果的差異進行反向傳播和權重更新，從而實現模型的訓練。

深度學習在近年來得到了廣泛的應用，包括圖像識別、語音識別、自然語言處理、推薦系統等。深度學習的應用前景非常廣泛，它已經在許多領域取得了顯著的成果，並為人類帶來了更多的智能化和便利化。


## 1.2 深度學習的發展史

深度學習的發展可以追溯到上世紀50年代的人工神經網絡（Artificial Neural Networks, ANN）。然而，由於當時計算能力的限制和缺乏大量的標註數據，ANN 的應用一直受到限制。

在20世紀90年代，支持向量機（Support Vector Machine, SVM）等新的機器學習方法崛起，使得深度學習逐漸被忽視。然而，2006年Hinton等人提出了深度信念網絡（Deep Belief Network, DBN），引起了學術界和產業界的廣泛關注。

此後，隨著計算能力的提升和大量的標註數據的出現，深度學習的發展得到了極大的推動。2012年，Hinton和他的團隊使用卷積神經網絡（Convolutional Neural Network, CNN）在ImageNet圖像識別競賽中取得了驚人的成果，使得深度學習受到了更廣泛的關注。

此後，深度學習在圖像識別、語音識別、自然語言處理等領域取得了突破性的進展，成為當今人工智能領域最具前景和應用價值的技術之一。其中，深度學習在圖像識別方面取得的成果最為顯著，例如 AlphaGo 和 AlphaZero 在圍棋和其他棋類遊戲上的勝利，以及DeepMind的StarCraft II 機器人在遊戲中戰勝人類職業選手。

深度學習的發展歷程中，Python 和 Pytorch 兩者皆有著重要的地位。Python 作為一種高級編程語言，具有簡潔、易讀、易用等優點，被廣泛用於深度學習的編程和開發。而 Pytorch 作為一個開源深度學習框架，提供了一系列強大的工具和函數，可以協助開發者更加高效地進行深度學習模型的開發和訓練。


## 1.3 深度學習的基本模型架構

深度學習的基本模型架構由多個層次的神經網絡構成，每層神經元的輸出被作為下一層的輸入，直至輸出層輸出最終的預測結果。深度學習的基本模型架構可以分為以下三種：

1. 前饋神經網絡（Feedforward Neural Network）

前饋神經網絡也被稱為多層感知器（Multilayer Perceptron, MLP），是最簡單的神經網絡模型。它由一個輸入層、多個隱藏層和一個輸出層組成，每層都包含多個神經元。前饋神經網絡的輸出是通過經過多個線性變換和非線性激活函數的組合得到的。前饋神經網絡被廣泛應用於圖像識別、語音識別、自然語言處理等領域。

2. 卷積神經網絡（Convolutional Neural Network）

卷積神經網絡是一種特殊的神經網絡，主要應用於圖像識別等領域。它使用卷積層和池化層替代了前饋神經網絡中的全連接層，能夠更好地處理圖像等數據。卷積層使用一個可學習的卷積核對輸入圖像進行卷積運算，提取圖像的特徵。池化層則用來減少特徵圖的大小，提高計算效率。

3. 長短期記憶網絡（Long Short-Term Memory, LSTM）

長短期記憶網絡是一種常用於自然語言處理和語音識別等領域的循環神經網絡模型。它通過記憶單元和遗忘閥門等機制實現了對長期依賴關係的建模。LSTM 的訓練通常使用反向傳遞算法（Backpropagation）進行，其訓練過程包括前向傳遞和反向傳遞兩個步驟，其中前向傳遞用於計算模型的預測值，反向傳遞用於計算損失函數對模型參數的梯度，進而進行模型的參數更新。

這三種基本模型架構都有不同的特點和應用領域，但它們都有相同的基本結構，即由多個神經元和層次組成的神經網絡。在實際應用中，根據不同的問題和數據特點，可以選擇適合的模型架構和參數設置，進行模型的訓練和預測。

## 1.4 Python + Pytorch

Python和PyTorch是深度學習領域中常用的程式語言和框架。本章節介紹Python和PyTorch的基本概念、語法和用法，以便讀者能夠理解和使用後續章節中的程式碼範例。

### 1.4.1 Python簡介

Python是一種高級程式語言，其簡潔的語法和豐富的函數庫使得Python成為科學計算、數據分析和機器學習等領域的主流語言之一。Python支持多種程式範式，包括面向對象、函數式和過程式等，也支持多種數據類型，如列表、字典、元組、集合和數組等。

下面是一個Python的"Hello, World!"程式範例：

```py
print("Hello, World!")
```

### 1.4.2 PyTorch簡介

PyTorch是一個基於Python的科學計算庫，其提供了豐富的深度學習功能和工具，支持動態圖和靜態圖的建模方式，並且具有高效、靈活和易用的特點。PyTorch的核心是Tensor，它是一種多維數組，類似於NumPy中的ndarray，但具有GPU加速等特性，可以快速進行大規模的數據運算。

下面是一個PyTorch的Tensor操作範例：

```py
import torch

# 創建一個2x3的Tensor
x = torch.tensor([[1, 2, 3], [4, 5, 6]])

# 創建一個3x2的Tensor
y = torch.tensor([[1, 2], [3, 4], [5, 6]])

# 計算x和y的矩陣乘法
z = torch.matmul(x, y)

# 輸出結果
print(z)

```

在PyTorch中，還可以使用自動微分機制來計算梯度，這是深度學習中常用的技術之一。PyTorch提供了torch.autograd模塊來實現自動微分，讀者可以在後續章節中進一步了解。

在使用PyTorch進行深度學習時，首先需要導入必要的庫和模塊，例如：

```py
import torch
import torch.nn as nn
import torch.optim as optim

```

其中，torch是PyTorch的核心庫，包含了大量的強大功能和工具；nn是PyTorch中的神經網路模塊，包含了各種常用的層和激活函數；optim是PyTorch中的優化器模塊，包含了各種常用的梯度下降算法和變種算法。

接下來，可以創建一個神經網路模型，例如：

```py
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(784, 512)
        self.fc2 = nn.Linear(512, 10)
    
    def forward(self, x):
        x = x.view(-1, 784)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

```

這個模型包含兩個全連接層，其中第一層的輸入維度為784，輸出維度為512；第二層的輸入維度為512，輸出維度為10。模型還定義了一個前向傳播的函數forward，用於計算模型的輸出。

接下來，可以定義一個損失函數和一個優化器，例如：

```py
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

```

其中，損失函數使用交叉熵損失，優化器使用SGD算法，學習率為0.001，動量為0.9。這些設置可以根據實際情況進行調整。

最後，可以進行模型的訓練和測試，例如：

```py
for epoch in range(10):
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    print('[%d] loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))

```

可以使用以下程式碼進行測試：

```py
correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy on test set: %d %%' % (100 * correct / total))

```

這個程式碼會對測試集中的數據進行預測，並計算模型的準確率。其中，torch.no_grad()用於關閉梯度計算，從而減少內存的使用。

以上就是PyTorch的基本用法。接下來的章節將介紹如何使用PyTorch進行梯度下降、反傳遞和Transformer等操作。